<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>🎯 Nara STT Test - Speech Recognition</title>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Roboto', -apple-system, BlinkMacSystemFont, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
            color: #333;
        }

        .main-container {
            max-width: 1000px;
            margin: 0 auto;
        }

        .header {
            text-align: center;
            margin-bottom: 40px;
            color: white;
        }

        .header h1 {
            font-size: 2.5rem;
            font-weight: 300;
            margin-bottom: 8px;
            letter-spacing: -0.5px;
        }

        .header p {
            font-size: 1.1rem;
            opacity: 0.9;
            font-weight: 400;
        }

        .container {
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(20px);
            border-radius: 20px;
            padding: 40px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
            border: 1px solid rgba(255, 255, 255, 0.2);
        }

        .status-card {
            background: white;
            border-radius: 16px;
            padding: 24px;
            margin-bottom: 32px;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
            border: 1px solid rgba(0, 0, 0, 0.05);
            transition: all 0.3s ease;
        }

        .status-card:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 30px rgba(0, 0, 0, 0.12);
        }

        .status-indicator {
            display: flex;
            align-items: center;
            gap: 16px;
            margin-bottom: 16px;
        }

        .status-dot {
            width: 12px;
            height: 12px;
            border-radius: 50%;
            background: #e0e0e0;
            transition: all 0.3s ease;
        }

        .status-dot.ready { background: #4CAF50; box-shadow: 0 0 0 4px rgba(76, 175, 80, 0.2); }
        .status-dot.listening {
            background: #2196F3;
            box-shadow: 0 0 0 4px rgba(33, 150, 243, 0.2);
            animation: pulse-blue 2s infinite;
        }
        .status-dot.error { background: #f44336; box-shadow: 0 0 0 4px rgba(244, 67, 54, 0.2); }
        .status-dot.processing {
            background: #FF9800;
            box-shadow: 0 0 0 4px rgba(255, 152, 0, 0.2);
            animation: pulse-orange 1.5s infinite;
        }

        @keyframes pulse-blue {
            0%, 100% { transform: scale(1); opacity: 1; }
            50% { transform: scale(1.1); opacity: 0.8; }
        }

        @keyframes pulse-orange {
            0%, 100% { transform: scale(1); opacity: 1; }
            50% { transform: scale(1.1); opacity: 0.8; }
        }

        .status-text {
            font-size: 1.1rem;
            font-weight: 500;
            color: #333;
        }

        .status-subtitle {
            font-size: 0.9rem;
            color: #666;
            margin-top: 4px;
        }

        .controls {
            display: flex;
            gap: 16px;
            justify-content: center;
            margin: 32px 0;
            flex-wrap: wrap;
        }

        .btn {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            padding: 16px 32px;
            border-radius: 12px;
            cursor: pointer;
            font-size: 1rem;
            font-weight: 500;
            font-family: 'Roboto', sans-serif;
            transition: all 0.3s ease;
            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3);
            min-width: 140px;
        }

        .btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 25px rgba(102, 126, 234, 0.4);
        }

        .btn:active {
            transform: translateY(0);
        }

        .btn:disabled {
            background: #e0e0e0;
            color: #999;
            cursor: not-allowed;
            box-shadow: none;
            transform: none;
        }

        .btn.danger {
            background: linear-gradient(135deg, #ff6b6b 0%, #ee5a52 100%);
            box-shadow: 0 4px 15px rgba(255, 107, 107, 0.3);
        }

        .btn.danger:hover {
            box-shadow: 0 8px 25px rgba(255, 107, 107, 0.4);
        }

        .btn.recording {
            animation: recording-pulse 1s infinite;
        }

        @keyframes recording-pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.7; }
        }

        .transcription-card {
            background: white;
            border-radius: 16px;
            padding: 32px;
            margin: 24px 0;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
            border: 1px solid rgba(0, 0, 0, 0.05);
            min-height: 120px;
            transition: all 0.3s ease;
        }

        .transcription-card.has-content {
            border-left: 4px solid #4CAF50;
        }

        .transcription-card.partial {
            border-left: 4px solid #FF9800;
            background: linear-gradient(135deg, #fff8e1 0%, #ffffff 100%);
        }

        .transcription-text {
            font-size: 1.2rem;
            line-height: 1.6;
            color: #333;
            font-weight: 400;
        }

        .transcription-placeholder {
            color: #999;
            font-style: italic;
            text-align: center;
            padding: 40px 0;
        }

        .config-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 24px;
            margin: 32px 0;
        }

        .config-card {
            background: white;
            border-radius: 12px;
            padding: 24px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.05);
            border: 1px solid rgba(0, 0, 0, 0.05);
        }

        .config-title {
            font-size: 1.1rem;
            font-weight: 600;
            color: #333;
            margin-bottom: 16px;
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .config-item {
            display: flex;
            justify-content: space-between;
            margin-bottom: 8px;
            font-size: 0.9rem;
        }

        .config-label {
            color: #666;
            font-weight: 500;
        }

        .config-value {
            color: #333;
            font-weight: 400;
        }

        .log-container {
            background: #1a1a1a;
            border-radius: 12px;
            padding: 24px;
            margin-top: 32px;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.15);
        }

        .log-header {
            color: #fff;
            font-size: 1.1rem;
            font-weight: 500;
            margin-bottom: 16px;
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .log {
            background: #000;
            border-radius: 8px;
            padding: 20px;
            max-height: 300px;
            overflow-y: auto;
            font-family: 'SF Mono', 'Monaco', 'Consolas', monospace;
            font-size: 13px;
            line-height: 1.5;
            color: #00ff41;
            white-space: pre-wrap;
            border: 1px solid #333;
        }

        .log::-webkit-scrollbar {
            width: 8px;
        }

        .log::-webkit-scrollbar-track {
            background: #1a1a1a;
            border-radius: 4px;
        }

        .log::-webkit-scrollbar-thumb {
            background: #444;
            border-radius: 4px;
        }

        .log::-webkit-scrollbar-thumb:hover {
            background: #555;
        }

        .audio-visualizer {
            width: 100%;
            height: 80px;
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            border-radius: 12px;
            margin: 24px 0;
            border: 1px solid rgba(0, 0, 0, 0.05);
        }

        .instructions {
            background: linear-gradient(135deg, #e3f2fd 0%, #ffffff 100%);
            border-radius: 12px;
            padding: 24px;
            margin-top: 32px;
            border: 1px solid rgba(33, 150, 243, 0.1);
        }

        .instructions h3 {
            color: #1976d2;
            font-size: 1.2rem;
            font-weight: 600;
            margin-bottom: 16px;
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .instructions ol {
            color: #333;
            line-height: 1.6;
            padding-left: 20px;
        }

        .instructions li {
            margin-bottom: 8px;
            font-weight: 400;
        }

        @media (max-width: 768px) {
            .container {
                padding: 24px;
                margin: 10px;
            }

            .header h1 {
                font-size: 2rem;
            }

            .controls {
                flex-direction: column;
                align-items: center;
            }

            .btn {
                width: 100%;
                max-width: 280px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>🎤 Vapi Speech-to-Text Test</h1>

        <div class="config">
            <h2>Configuration</h2>
            <div class="config-item">🔑 Vapi API Key: 765f8644-****-****-****-************</div>
            <div class="config-item">🤖 Assistant: Nara STT-Only (Silent Transcription)</div>
            <div class="config-item">🎯 Model: GPT-4o Transcribe (OpenAI)</div>
            <div class="config-item">🌍 Language: en-US</div>
        </div>

        <div id="status" class="status ready">
            🟢 Ready to test speech recognition
        </div>

        <div class="controls">
            <button id="startListening" onclick="startListening()">🎤 Start Listening</button>
            <button id="stopListening" onclick="stopListening()" disabled>⏹️ Stop Listening</button>
            <button id="testMicrophone" onclick="testMicrophone()">🔧 Test Microphone</button>
            <button id="clearTranscription" onclick="clearTranscription()">🗑️ Clear</button>
        </div>

        <h2>Microphone Level</h2>
        <div class="mic-level">
            <div id="micLevelBar" class="mic-level-bar"></div>
        </div>

        <h2>Live Transcription</h2>
        <div id="transcription" class="transcription">
            Say something to test speech recognition...
        </div>

        <h2>Audio Visualizer</h2>
        <canvas id="audioVisualizer" class="audio-visualizer" width="800" height="60"></canvas>

        <h2>Debug Log</h2>
        <div id="log" class="log">Ready to start speech recognition...\n</div>
    </div>

    <script>
        // Configuration
        const CONFIG = {
            vapi: {
                apiKey: '765f8644-1464-4b36-a4fe-c660e15ba313',
                assistantId: '0bfc6364-690a-492b-9671-a109c5937342' // Nara STT-Only
            }
        };

        let mediaStream = null;
        let audioContext = null;
        let analyser = null;
        let microphone = null;
        let vapiWebSocket = null;
        let isListening = false;
        let animationId = null;

        function log(message) {
            const logElement = document.getElementById('log');
            const timestamp = new Date().toLocaleTimeString();
            logElement.textContent += `[${timestamp}] ${message}\n`;
            logElement.scrollTop = logElement.scrollHeight;
        }

        function setStatus(message, type = 'ready') {
            const statusElement = document.getElementById('status');
            statusElement.textContent = message;
            statusElement.className = `status ${type}`;
        }

        function updateTranscription(text, isFinal = false) {
            const transcriptionElement = document.getElementById('transcription');
            transcriptionElement.textContent = text;
            transcriptionElement.className = `transcription ${isFinal ? 'final' : 'partial'}`;
        }

        function clearTranscription() {
            updateTranscription('Say something to test speech recognition...');
            document.getElementById('transcription').className = 'transcription';
        }

        async function testMicrophone() {
            log('🔧 Testing microphone access...');
            setStatus('🔄 Testing microphone...', 'processing');

            try {
                const stream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        sampleRate: 16000,
                        channelCount: 1,
                        echoCancellation: true,
                        noiseSuppression: true
                    }
                });

                log('✅ Microphone access granted');
                log(`   Tracks: ${stream.getAudioTracks().length}`);

                const track = stream.getAudioTracks()[0];
                const settings = track.getSettings();
                log(`   Sample Rate: ${settings.sampleRate}Hz`);
                log(`   Channels: ${settings.channelCount}`);

                // Test audio level detection
                setupAudioVisualization(stream);

                setTimeout(() => {
                    stream.getTracks().forEach(track => track.stop());
                    setStatus('✅ Microphone test complete', 'ready');
                }, 3000);

            } catch (error) {
                log(`❌ Microphone test failed: ${error.message}`);
                setStatus('❌ Microphone access denied', 'error');
            }
        }

        function setupAudioVisualization(stream) {
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            analyser = audioContext.createAnalyser();
            microphone = audioContext.createMediaStreamSource(stream);

            analyser.fftSize = 256;
            microphone.connect(analyser);

            const canvas = document.getElementById('audioVisualizer');
            const ctx = canvas.getContext('2d');
            const bufferLength = analyser.frequencyBinCount;
            const dataArray = new Uint8Array(bufferLength);

            function draw() {
                if (!isListening && !mediaStream) return;

                animationId = requestAnimationFrame(draw);

                analyser.getByteFrequencyData(dataArray);

                // Calculate average volume
                let sum = 0;
                for (let i = 0; i < bufferLength; i++) {
                    sum += dataArray[i];
                }
                const average = sum / bufferLength;

                // Update mic level bar
                const micLevelBar = document.getElementById('micLevelBar');
                const percentage = (average / 255) * 100;
                micLevelBar.style.width = `${percentage}%`;

                // Draw waveform
                ctx.fillStyle = '#f8f9fa';
                ctx.fillRect(0, 0, canvas.width, canvas.height);

                ctx.lineWidth = 2;
                ctx.strokeStyle = '#007bff';
                ctx.beginPath();

                const sliceWidth = canvas.width / bufferLength;
                let x = 0;

                for (let i = 0; i < bufferLength; i++) {
                    const v = dataArray[i] / 128.0;
                    const y = v * canvas.height / 2;

                    if (i === 0) {
                        ctx.moveTo(x, y);
                    } else {
                        ctx.lineTo(x, y);
                    }

                    x += sliceWidth;
                }

                ctx.lineTo(canvas.width, canvas.height / 2);
                ctx.stroke();
            }

            draw();
        }

        async function startListening() {
            log('🎤 Starting speech recognition...');
            setStatus('🔄 Initializing...', 'processing');

            try {
                // Step 1: Get microphone access
                log('1. Requesting microphone access...');
                mediaStream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        sampleRate: 16000,
                        channelCount: 1,
                        echoCancellation: true,
                        noiseSuppression: true
                    }
                });
                log('✅ Microphone access granted');

                // Step 2: Setup audio visualization
                setupAudioVisualization(mediaStream);

                // Step 3: Create Vapi WebSocket call
                log('2. Creating Vapi call...');
                const callResponse = await fetch('https://api.vapi.ai/call', {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${CONFIG.vapi.apiKey}`,
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({
                        assistantId: CONFIG.vapi.assistantId,
                        transport: {
                            provider: 'vapi.websocket',
                            audioFormat: {
                                format: 'pcm_s16le',
                                container: 'raw',
                                sampleRate: 16000
                            }
                        }
                    })
                });

                if (!callResponse.ok) {
                    throw new Error(`Call creation failed: ${callResponse.status}`);
                }

                const callData = await callResponse.json();
                log(`✅ Call created: ${callData.id}`);

                // Step 4: Connect WebSocket
                log('3. Connecting to Vapi WebSocket...');
                vapiWebSocket = new WebSocket(callData.transport.websocketCallUrl);

                vapiWebSocket.onopen = () => {
                    log('✅ WebSocket connected');
                    setStatus('👂 Listening for speech...', 'listening');
                    isListening = true;

                    document.getElementById('startListening').disabled = true;
                    document.getElementById('stopListening').disabled = false;
                    document.getElementById('startListening').textContent = '🎤 Listening...';
                    document.getElementById('startListening').className = 'recording';

                    // Start sending audio data
                    startAudioStreaming();
                };

                vapiWebSocket.onmessage = (event) => {
                    try {
                        const message = JSON.parse(event.data);
                        handleVapiMessage(message);
                    } catch (e) {
                        // Binary audio data - ignore for STT test
                    }
                };

                vapiWebSocket.onerror = (error) => {
                    log(`❌ WebSocket error: ${error}`);
                    setStatus('❌ Connection failed', 'error');
                    stopListening();
                };

                vapiWebSocket.onclose = () => {
                    log('🔌 WebSocket disconnected');
                    if (isListening) {
                        setStatus('🔌 Connection closed', 'ready');
                        stopListening();
                    }
                };

            } catch (error) {
                log(`❌ Failed to start listening: ${error.message}`);
                setStatus('❌ Failed to start', 'error');
                stopListening();
            }
        }

                        function handleVapiMessage(message) {
            // Log the full message for debugging (but truncate if too long)
            const messageStr = JSON.stringify(message);
            const truncatedMessage = messageStr.length > 200 ? messageStr.substring(0, 200) + '...' : messageStr;
            log(`📥 Raw message: ${truncatedMessage}`);

            switch (message.type) {
                                case 'transcript':
                    // ONLY show USER transcripts, ignore assistant responses
                    if (message.role === 'assistant') {
                        log(`🤖 Assistant transcript (ignored): "${message.transcript || message.text}"`);
                        break;
                    }

                    // Handle USER transcript formats only
                    let text = 'No text';
                    let isFinal = false;

                    // OpenAI format
                    if (message.transcript) {
                        text = message.transcript;
                        isFinal = message.transcriptType === 'final';
                    }
                    // Deepgram format
                    else if (message.text) {
                        text = message.text;
                        isFinal = message.is_final || message.isFinal || false;
                    }
                    // Alternative formats
                    else if (message.transcriptPartial) {
                        text = message.transcriptPartial;
                        isFinal = false;
                    }
                    else if (message.transcriptFinal) {
                        text = message.transcriptFinal;
                        isFinal = true;
                    }

                    log(`📝 USER Transcript: "${text}" (${isFinal ? 'final' : 'partial'})`);
                    updateTranscription(text, isFinal);
                    break;

                case 'speech-update':
                    const status = message.status || message.speech_status;
                    if (status === 'started' || status === 'speaking') {
                        log('🗣️ Speech detected');
                        setStatus('🗣️ Processing speech...', 'processing');
                    } else if (status === 'stopped' || status === 'ended') {
                        log('🤐 Speech ended');
                        setStatus('👂 Listening for speech...', 'listening');
                    }
                    log(`🔊 Speech status: ${status}`);
                    break;

                case 'conversation-update':
                    log(`💬 Conversation: ${message.status || message.conversation_status || 'unknown'}`);
                    break;

                case 'status-update':
                    log(`📊 Status: ${message.status || JSON.stringify(message)}`);
                    break;

                case 'user-interrupted':
                    log('⚠️ User interrupted');
                    break;

                case 'model-output':
                    // Suppress model output spam - we only want STT
                    // log(`🤖 Model output (suppressed): ${message.output}`);
                    break;

                default:
                    log(`📥 Unknown message type: ${message.type}`);
                    log(`   Content: ${JSON.stringify(message)}`);
            }
        }

                function startAudioStreaming() {
            if (!mediaStream || !vapiWebSocket) return;

            // Create audio context with 16kHz sample rate for Vapi
            const audioContext = new (window.AudioContext || window.webkitAudioContext)({
                sampleRate: 16000
            });

            log(`📊 Audio context sample rate: ${audioContext.sampleRate}Hz`);

            const source = audioContext.createMediaStreamSource(mediaStream);
            const processor = audioContext.createScriptProcessor(4096, 1, 1);

            processor.onaudioprocess = (event) => {
                if (vapiWebSocket && vapiWebSocket.readyState === WebSocket.OPEN) {
                    const inputBuffer = event.inputBuffer.getChannelData(0);

                    // Convert Float32Array to Int16Array (PCM)
                    const pcmBuffer = new Int16Array(inputBuffer.length);
                    for (let i = 0; i < inputBuffer.length; i++) {
                        // Clamp values to 16-bit range
                        const sample = Math.max(-1, Math.min(1, inputBuffer[i]));
                        pcmBuffer[i] = sample * 32767;
                    }

                    // Send PCM data to Vapi
                    try {
                        vapiWebSocket.send(pcmBuffer.buffer);
                    } catch (error) {
                        log(`❌ Error sending audio: ${error.message}`);
                    }
                }
            };

            source.connect(processor);
            processor.connect(audioContext.destination);

            // Store for cleanup
            window.audioProcessor = processor;
            window.audioSource = source;
            window.streamAudioContext = audioContext;
        }

        function stopListening() {
            log('⏹️ Stopping speech recognition...');

            isListening = false;

            // Stop WebSocket
            if (vapiWebSocket) {
                vapiWebSocket.close();
                vapiWebSocket = null;
            }

            // Stop microphone
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                mediaStream = null;
            }

            // Stop audio processing
            if (window.audioProcessor) {
                window.audioProcessor.disconnect();
                window.audioSource.disconnect();
            }

            // Close audio context
            if (window.streamAudioContext) {
                window.streamAudioContext.close();
            }

            // Stop visualization
            if (animationId) {
                cancelAnimationFrame(animationId);
                animationId = null;
            }

            // Reset UI
            document.getElementById('startListening').disabled = false;
            document.getElementById('stopListening').disabled = true;
            document.getElementById('startListening').textContent = '🎤 Start Listening';
            document.getElementById('startListening').className = '';

            // Reset mic level
            document.getElementById('micLevelBar').style.width = '0%';

            setStatus('🟢 Ready to test speech recognition', 'ready');
            log('✅ Stopped listening');
        }

        // Cleanup on page unload
        window.addEventListener('beforeunload', () => {
            stopListening();
        });

        // Initial log
        log('🎤 Speech-to-Text Test Ready');
        log('Click "Start Listening" to begin speech recognition');
        log('Try saying: "Hello, this is a test of speech recognition"');
    </script>
</body>
</html>

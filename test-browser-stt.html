<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Vapi STT (Speech-to-Text) Test</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            background: #f5f5f5;
        }
        .container {
            background: white;
            padding: 30px;
            border-radius: 12px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        .status {
            padding: 15px;
            border-radius: 8px;
            margin: 10px 0;
            font-weight: 500;
        }
        .status.ready { background: #d4edda; color: #155724; border: 1px solid #c3e6cb; }
        .status.error { background: #f8d7da; color: #721c24; border: 1px solid #f5c6cb; }
        .status.listening { background: #cce5ff; color: #004085; border: 1px solid #b3d7ff; }
        .status.processing { background: #fff3cd; color: #856404; border: 1px solid #ffeaa7; }

        button {
            background: #007bff;
            color: white;
            border: none;
            padding: 12px 24px;
            border-radius: 6px;
            cursor: pointer;
            font-size: 16px;
            margin: 5px;
            transition: background 0.2s;
        }
        button:hover { background: #0056b3; }
        button:disabled { background: #6c757d; cursor: not-allowed; }
        button.recording { background: #dc3545; animation: pulse 1s infinite; }

        @keyframes pulse {
            0% { opacity: 1; }
            50% { opacity: 0.7; }
            100% { opacity: 1; }
        }

        .controls {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            margin: 20px 0;
        }

        .transcription {
            background: #f8f9fa;
            border: 2px solid #dee2e6;
            border-radius: 8px;
            padding: 20px;
            margin: 15px 0;
            min-height: 100px;
            font-size: 18px;
            line-height: 1.5;
        }

        .transcription.final {
            border-color: #28a745;
            background: #d4edda;
        }

        .transcription.partial {
            border-color: #ffc107;
            background: #fff3cd;
        }

        .log {
            background: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 6px;
            padding: 15px;
            max-height: 300px;
            overflow-y: auto;
            font-family: 'Monaco', 'Consolas', monospace;
            font-size: 12px;
            white-space: pre-wrap;
        }

        .audio-visualizer {
            width: 100%;
            height: 60px;
            background: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 6px;
            margin: 10px 0;
        }

        .config {
            background: #e9ecef;
            padding: 15px;
            border-radius: 6px;
            margin: 15px 0;
        }

        .config-item {
            margin: 5px 0;
            font-family: monospace;
        }

        h1 { color: #333; text-align: center; }
        h2 { color: #555; border-bottom: 2px solid #007bff; padding-bottom: 5px; }

        .mic-level {
            width: 100%;
            height: 20px;
            background: #e9ecef;
            border-radius: 10px;
            overflow: hidden;
            margin: 10px 0;
        }

        .mic-level-bar {
            height: 100%;
            background: linear-gradient(90deg, #28a745, #ffc107, #dc3545);
            width: 0%;
            transition: width 0.1s;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üé§ Vapi Speech-to-Text Test</h1>

        <div class="config">
            <h2>Configuration</h2>
            <div class="config-item">üîë Vapi API Key: 765f8644-****-****-****-************</div>
            <div class="config-item">ü§ñ Assistant: Nara STT-Only (Silent Transcription)</div>
            <div class="config-item">üéØ Model: GPT-4o Transcribe (OpenAI)</div>
            <div class="config-item">üåç Language: en-US</div>
        </div>

        <div id="status" class="status ready">
            üü¢ Ready to test speech recognition
        </div>

        <div class="controls">
            <button id="startListening" onclick="startListening()">üé§ Start Listening</button>
            <button id="stopListening" onclick="stopListening()" disabled>‚èπÔ∏è Stop Listening</button>
            <button id="testMicrophone" onclick="testMicrophone()">üîß Test Microphone</button>
            <button id="clearTranscription" onclick="clearTranscription()">üóëÔ∏è Clear</button>
        </div>

        <h2>Microphone Level</h2>
        <div class="mic-level">
            <div id="micLevelBar" class="mic-level-bar"></div>
        </div>

        <h2>Live Transcription</h2>
        <div id="transcription" class="transcription">
            Say something to test speech recognition...
        </div>

        <h2>Audio Visualizer</h2>
        <canvas id="audioVisualizer" class="audio-visualizer" width="800" height="60"></canvas>

        <h2>Debug Log</h2>
        <div id="log" class="log">Ready to start speech recognition...\n</div>
    </div>

    <script>
        // Configuration
        const CONFIG = {
            vapi: {
                apiKey: '765f8644-1464-4b36-a4fe-c660e15ba313',
                assistantId: '0bfc6364-690a-492b-9671-a109c5937342' // Nara STT-Only
            }
        };

        let mediaStream = null;
        let audioContext = null;
        let analyser = null;
        let microphone = null;
        let vapiWebSocket = null;
        let isListening = false;
        let animationId = null;

        function log(message) {
            const logElement = document.getElementById('log');
            const timestamp = new Date().toLocaleTimeString();
            logElement.textContent += `[${timestamp}] ${message}\n`;
            logElement.scrollTop = logElement.scrollHeight;
        }

        function setStatus(message, type = 'ready') {
            const statusElement = document.getElementById('status');
            statusElement.textContent = message;
            statusElement.className = `status ${type}`;
        }

        function updateTranscription(text, isFinal = false) {
            const transcriptionElement = document.getElementById('transcription');
            transcriptionElement.textContent = text;
            transcriptionElement.className = `transcription ${isFinal ? 'final' : 'partial'}`;
        }

        function clearTranscription() {
            updateTranscription('Say something to test speech recognition...');
            document.getElementById('transcription').className = 'transcription';
        }

        async function testMicrophone() {
            log('üîß Testing microphone access...');
            setStatus('üîÑ Testing microphone...', 'processing');

            try {
                const stream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        sampleRate: 16000,
                        channelCount: 1,
                        echoCancellation: true,
                        noiseSuppression: true
                    }
                });

                log('‚úÖ Microphone access granted');
                log(`   Tracks: ${stream.getAudioTracks().length}`);

                const track = stream.getAudioTracks()[0];
                const settings = track.getSettings();
                log(`   Sample Rate: ${settings.sampleRate}Hz`);
                log(`   Channels: ${settings.channelCount}`);

                // Test audio level detection
                setupAudioVisualization(stream);

                setTimeout(() => {
                    stream.getTracks().forEach(track => track.stop());
                    setStatus('‚úÖ Microphone test complete', 'ready');
                }, 3000);

            } catch (error) {
                log(`‚ùå Microphone test failed: ${error.message}`);
                setStatus('‚ùå Microphone access denied', 'error');
            }
        }

        function setupAudioVisualization(stream) {
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            analyser = audioContext.createAnalyser();
            microphone = audioContext.createMediaStreamSource(stream);

            analyser.fftSize = 256;
            microphone.connect(analyser);

            const canvas = document.getElementById('audioVisualizer');
            const ctx = canvas.getContext('2d');
            const bufferLength = analyser.frequencyBinCount;
            const dataArray = new Uint8Array(bufferLength);

            function draw() {
                if (!isListening && !mediaStream) return;

                animationId = requestAnimationFrame(draw);

                analyser.getByteFrequencyData(dataArray);

                // Calculate average volume
                let sum = 0;
                for (let i = 0; i < bufferLength; i++) {
                    sum += dataArray[i];
                }
                const average = sum / bufferLength;

                // Update mic level bar
                const micLevelBar = document.getElementById('micLevelBar');
                const percentage = (average / 255) * 100;
                micLevelBar.style.width = `${percentage}%`;

                // Draw waveform
                ctx.fillStyle = '#f8f9fa';
                ctx.fillRect(0, 0, canvas.width, canvas.height);

                ctx.lineWidth = 2;
                ctx.strokeStyle = '#007bff';
                ctx.beginPath();

                const sliceWidth = canvas.width / bufferLength;
                let x = 0;

                for (let i = 0; i < bufferLength; i++) {
                    const v = dataArray[i] / 128.0;
                    const y = v * canvas.height / 2;

                    if (i === 0) {
                        ctx.moveTo(x, y);
                    } else {
                        ctx.lineTo(x, y);
                    }

                    x += sliceWidth;
                }

                ctx.lineTo(canvas.width, canvas.height / 2);
                ctx.stroke();
            }

            draw();
        }

        async function startListening() {
            log('üé§ Starting speech recognition...');
            setStatus('üîÑ Initializing...', 'processing');

            try {
                // Step 1: Get microphone access
                log('1. Requesting microphone access...');
                mediaStream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        sampleRate: 16000,
                        channelCount: 1,
                        echoCancellation: true,
                        noiseSuppression: true
                    }
                });
                log('‚úÖ Microphone access granted');

                // Step 2: Setup audio visualization
                setupAudioVisualization(mediaStream);

                // Step 3: Create Vapi WebSocket call
                log('2. Creating Vapi call...');
                const callResponse = await fetch('https://api.vapi.ai/call', {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${CONFIG.vapi.apiKey}`,
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({
                        assistantId: CONFIG.vapi.assistantId,
                        transport: {
                            provider: 'vapi.websocket',
                            audioFormat: {
                                format: 'pcm_s16le',
                                container: 'raw',
                                sampleRate: 16000
                            }
                        }
                    })
                });

                if (!callResponse.ok) {
                    throw new Error(`Call creation failed: ${callResponse.status}`);
                }

                const callData = await callResponse.json();
                log(`‚úÖ Call created: ${callData.id}`);

                // Step 4: Connect WebSocket
                log('3. Connecting to Vapi WebSocket...');
                vapiWebSocket = new WebSocket(callData.transport.websocketCallUrl);

                vapiWebSocket.onopen = () => {
                    log('‚úÖ WebSocket connected');
                    setStatus('üëÇ Listening for speech...', 'listening');
                    isListening = true;

                    document.getElementById('startListening').disabled = true;
                    document.getElementById('stopListening').disabled = false;
                    document.getElementById('startListening').textContent = 'üé§ Listening...';
                    document.getElementById('startListening').className = 'recording';

                    // Start sending audio data
                    startAudioStreaming();
                };

                vapiWebSocket.onmessage = (event) => {
                    try {
                        const message = JSON.parse(event.data);
                        handleVapiMessage(message);
                    } catch (e) {
                        // Binary audio data - ignore for STT test
                    }
                };

                vapiWebSocket.onerror = (error) => {
                    log(`‚ùå WebSocket error: ${error}`);
                    setStatus('‚ùå Connection failed', 'error');
                    stopListening();
                };

                vapiWebSocket.onclose = () => {
                    log('üîå WebSocket disconnected');
                    if (isListening) {
                        setStatus('üîå Connection closed', 'ready');
                        stopListening();
                    }
                };

            } catch (error) {
                log(`‚ùå Failed to start listening: ${error.message}`);
                setStatus('‚ùå Failed to start', 'error');
                stopListening();
            }
        }

                        function handleVapiMessage(message) {
            // Log the full message for debugging (but truncate if too long)
            const messageStr = JSON.stringify(message);
            const truncatedMessage = messageStr.length > 200 ? messageStr.substring(0, 200) + '...' : messageStr;
            log(`üì• Raw message: ${truncatedMessage}`);

            switch (message.type) {
                                case 'transcript':
                    // ONLY show USER transcripts, ignore assistant responses
                    if (message.role === 'assistant') {
                        log(`ü§ñ Assistant transcript (ignored): "${message.transcript || message.text}"`);
                        break;
                    }

                    // Handle USER transcript formats only
                    let text = 'No text';
                    let isFinal = false;

                    // OpenAI format
                    if (message.transcript) {
                        text = message.transcript;
                        isFinal = message.transcriptType === 'final';
                    }
                    // Deepgram format
                    else if (message.text) {
                        text = message.text;
                        isFinal = message.is_final || message.isFinal || false;
                    }
                    // Alternative formats
                    else if (message.transcriptPartial) {
                        text = message.transcriptPartial;
                        isFinal = false;
                    }
                    else if (message.transcriptFinal) {
                        text = message.transcriptFinal;
                        isFinal = true;
                    }

                    log(`üìù USER Transcript: "${text}" (${isFinal ? 'final' : 'partial'})`);
                    updateTranscription(text, isFinal);
                    break;

                case 'speech-update':
                    const status = message.status || message.speech_status;
                    if (status === 'started' || status === 'speaking') {
                        log('üó£Ô∏è Speech detected');
                        setStatus('üó£Ô∏è Processing speech...', 'processing');
                    } else if (status === 'stopped' || status === 'ended') {
                        log('ü§ê Speech ended');
                        setStatus('üëÇ Listening for speech...', 'listening');
                    }
                    log(`üîä Speech status: ${status}`);
                    break;

                case 'conversation-update':
                    log(`üí¨ Conversation: ${message.status || message.conversation_status || 'unknown'}`);
                    break;

                case 'status-update':
                    log(`üìä Status: ${message.status || JSON.stringify(message)}`);
                    break;

                case 'user-interrupted':
                    log('‚ö†Ô∏è User interrupted');
                    break;

                case 'model-output':
                    // Suppress model output spam - we only want STT
                    // log(`ü§ñ Model output (suppressed): ${message.output}`);
                    break;

                default:
                    log(`üì• Unknown message type: ${message.type}`);
                    log(`   Content: ${JSON.stringify(message)}`);
            }
        }

                function startAudioStreaming() {
            if (!mediaStream || !vapiWebSocket) return;

            // Create audio context with 16kHz sample rate for Vapi
            const audioContext = new (window.AudioContext || window.webkitAudioContext)({
                sampleRate: 16000
            });

            log(`üìä Audio context sample rate: ${audioContext.sampleRate}Hz`);

            const source = audioContext.createMediaStreamSource(mediaStream);
            const processor = audioContext.createScriptProcessor(4096, 1, 1);

            processor.onaudioprocess = (event) => {
                if (vapiWebSocket && vapiWebSocket.readyState === WebSocket.OPEN) {
                    const inputBuffer = event.inputBuffer.getChannelData(0);

                    // Convert Float32Array to Int16Array (PCM)
                    const pcmBuffer = new Int16Array(inputBuffer.length);
                    for (let i = 0; i < inputBuffer.length; i++) {
                        // Clamp values to 16-bit range
                        const sample = Math.max(-1, Math.min(1, inputBuffer[i]));
                        pcmBuffer[i] = sample * 32767;
                    }

                    // Send PCM data to Vapi
                    try {
                        vapiWebSocket.send(pcmBuffer.buffer);
                    } catch (error) {
                        log(`‚ùå Error sending audio: ${error.message}`);
                    }
                }
            };

            source.connect(processor);
            processor.connect(audioContext.destination);

            // Store for cleanup
            window.audioProcessor = processor;
            window.audioSource = source;
            window.streamAudioContext = audioContext;
        }

        function stopListening() {
            log('‚èπÔ∏è Stopping speech recognition...');

            isListening = false;

            // Stop WebSocket
            if (vapiWebSocket) {
                vapiWebSocket.close();
                vapiWebSocket = null;
            }

            // Stop microphone
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                mediaStream = null;
            }

            // Stop audio processing
            if (window.audioProcessor) {
                window.audioProcessor.disconnect();
                window.audioSource.disconnect();
            }

            // Close audio context
            if (window.streamAudioContext) {
                window.streamAudioContext.close();
            }

            // Stop visualization
            if (animationId) {
                cancelAnimationFrame(animationId);
                animationId = null;
            }

            // Reset UI
            document.getElementById('startListening').disabled = false;
            document.getElementById('stopListening').disabled = true;
            document.getElementById('startListening').textContent = 'üé§ Start Listening';
            document.getElementById('startListening').className = '';

            // Reset mic level
            document.getElementById('micLevelBar').style.width = '0%';

            setStatus('üü¢ Ready to test speech recognition', 'ready');
            log('‚úÖ Stopped listening');
        }

        // Cleanup on page unload
        window.addEventListener('beforeunload', () => {
            stopListening();
        });

        // Initial log
        log('üé§ Speech-to-Text Test Ready');
        log('Click "Start Listening" to begin speech recognition');
        log('Try saying: "Hello, this is a test of speech recognition"');
    </script>
</body>
</html>

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>üéØ Nara - Real Services Test</title>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Roboto', -apple-system, BlinkMacSystemFont, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
            color: #333;
        }

        .main-container {
            max-width: 1200px;
            margin: 0 auto;
        }

        .header {
            text-align: center;
            margin-bottom: 32px;
            color: white;
        }

        .header h1 {
            font-size: 2.5rem;
            font-weight: 300;
            margin-bottom: 8px;
            letter-spacing: -0.5px;
        }

        .header p {
            font-size: 1.1rem;
            opacity: 0.9;
            font-weight: 400;
        }

        .container {
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(20px);
            border-radius: 20px;
            padding: 40px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
            border: 1px solid rgba(255, 255, 255, 0.2);
        }

        .status-card {
            background: white;
            border-radius: 16px;
            padding: 24px;
            margin-bottom: 24px;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
            border: 1px solid rgba(0, 0, 0, 0.05);
            transition: all 0.3s ease;
        }

        .status-card:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 30px rgba(0, 0, 0, 0.12);
        }

        .status-indicator {
            display: flex;
            align-items: center;
            gap: 12px;
            font-weight: 500;
            color: #333;
        }

        .status-dot {
            width: 10px;
            height: 10px;
            border-radius: 50%;
            background: #e0e0e0;
        }

        .status-dot.ready { background: #4CAF50; }
        .status-dot.listening { background: #2196F3; animation: pulse 2s infinite; }
        .status-dot.processing { background: #FF9800; }
        .status-dot.error { background: #f44336; }

        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.6; }
        }

        .controls {
            display: flex;
            gap: 16px;
            justify-content: center;
            margin: 32px 0;
            flex-wrap: wrap;
        }

        .btn {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            padding: 16px 32px;
            border-radius: 12px;
            cursor: pointer;
            font-size: 1rem;
            font-weight: 500;
            font-family: 'Roboto', sans-serif;
            transition: all 0.3s ease;
            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3);
            min-width: 140px;
        }

        .btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 25px rgba(102, 126, 234, 0.4);
        }

        .btn:disabled {
            background: #e0e0e0;
            color: #999;
            cursor: not-allowed;
            box-shadow: none;
            transform: none;
        }

        .btn.danger {
            background: linear-gradient(135deg, #ff6b6b 0%, #ee5a52 100%);
            box-shadow: 0 4px 15px rgba(255, 107, 107, 0.3);
        }

        .btn.danger:hover {
            box-shadow: 0 8px 25px rgba(255, 107, 107, 0.4);
        }

        .transcript-card {
            background: white;
            border-radius: 16px;
            padding: 24px;
            margin: 16px 0;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.05);
            border: 1px solid rgba(0, 0, 0, 0.05);
            min-height: 80px;
        }

        .transcript-card.has-content {
            border-left: 4px solid #4CAF50;
        }

        .transcript-text {
            font-size: 1.1rem;
            line-height: 1.5;
            color: #333;
        }

        .transcript-placeholder {
            color: #999;
            font-style: italic;
            text-align: center;
            padding: 20px 0;
        }

        .log-container {
            background: #1a1a1a;
            border-radius: 12px;
            padding: 20px;
            margin-top: 24px;
        }

        .log-header {
            color: #fff;
            font-weight: 500;
            margin-bottom: 12px;
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .log {
            background: #000;
            border-radius: 8px;
            padding: 16px;
            max-height: 300px;
            overflow-y: auto;
            font-family: 'SF Mono', 'Monaco', 'Consolas', monospace;
            font-size: 12px;
            line-height: 1.4;
            color: #00ff41;
            white-space: pre-wrap;
        }

        .config-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 16px;
            margin: 24px 0;
        }

        .config-card {
            background: white;
            border-radius: 12px;
            padding: 20px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.05);
        }

        .config-title {
            font-weight: 600;
            color: #333;
            margin-bottom: 12px;
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .config-item {
            display: flex;
            justify-content: space-between;
            margin-bottom: 6px;
            font-size: 0.9rem;
        }

        .config-label {
            color: #666;
            font-weight: 500;
        }

        .config-value {
            color: #333;
        }

        @media (max-width: 768px) {
            .container {
                padding: 24px;
                margin: 10px;
            }

            .controls {
                flex-direction: column;
                align-items: center;
            }

            .btn {
                width: 100%;
                max-width: 280px;
            }
        }
            font-weight: bold;
        }
        .error {
            color: #ff6b6b;
            font-weight: bold;
        }
        .success {
            color: #51cf66;
            font-weight: bold;
        }
        .warning {
            color: #ffd43b;
            font-weight: bold;
        }
    </style>
</head>
<body>
    <div class="main-container">
        <div class="header">
            <h1>üéØ Nara Real Services Test</h1>
            <p>Complete STT ‚Üí AI ‚Üí TTS Pipeline with Real APIs</p>
        </div>

        <div class="container">
            <div class="status-card">
                <div class="status-indicator">
                    <div id="statusDot" class="status-dot ready"></div>
                    <span id="status">Ready to test real services</span>
                </div>
            </div>

            <div class="controls">
                <button class="btn" id="startBtn" onclick="startVoicePipeline()">üé§ Start Voice Pipeline</button>
                <button class="btn danger" id="stopBtn" onclick="stopVoicePipeline()" disabled>‚èπÔ∏è Stop Pipeline</button>
                <button class="btn" id="testTTSBtn" onclick="testTTS()">üó£Ô∏è Test TTS</button>
                <button class="btn" onclick="clearLog()">üßπ Clear Log</button>
            </div>

            <div class="transcript-card" id="transcriptCard">
                <div id="transcriptDisplay" class="transcript-placeholder">
                    Speak to see live transcription and AI responses...
                </div>
            </div>

            <div class="config-grid">
                <div class="config-card">
                    <div class="config-title">üéØ Vapi Configuration</div>
                    <div class="config-item">
                        <span class="config-label">Assistant:</span>
                        <span class="config-value">Nara STT-Only</span>
                    </div>
                    <div class="config-item">
                        <span class="config-label">Transcriber:</span>
                        <span class="config-value">GPT-4o Transcribe (OpenAI)</span>
                    </div>
                    <div class="config-item">
                        <span class="config-label">Sample Rate:</span>
                        <span class="config-value">16kHz PCM</span>
                    </div>
                </div>

                <div class="config-card">
                    <div class="config-title">üó£Ô∏è ElevenLabs TTS</div>
                    <div class="config-item">
                        <span class="config-label">Voice:</span>
                        <span class="config-value">Real Voice (XfWTl5ev8ylYnkKBEqnB)</span>
                    </div>
                    <div class="config-item">
                        <span class="config-label">Model:</span>
                        <span class="config-value">eleven_monolingual_v1</span>
                    </div>
                    <div class="config-item">
                        <span class="config-label">Services:</span>
                        <span class="config-value">Real VapiService.ts & TTSService.ts</span>
                    </div>
                </div>
            </div>

            <div class="log-container">
                <div class="log-header">üêõ Debug Log</div>
                <div class="log" id="log"></div>
            </div>
        </div>
    </div>

    <!-- Import your real services -->
    <script type="module">
        // This will be a module that imports your actual services
        // For now, we'll simulate the imports and create a bridge

        let vapiService = null;
        let ttsService = null;
        let isListening = false;

        // Configuration from your real config.ts
        const CONFIG = {
            vapi: {
                apiKey: '765f8644-1464-4b36-a4fe-c660e15ba313',
                assistantId: '0bfc6364-690a-492b-9671-a109c5937342'
            },
            elevenlabs: {
                apiKey: 'sk_536c3f9ad29e9e6e4f0b4aee762afa6d8db7d750d7f64587',     // Real API key from config.ts
                voiceId: 'XfWTl5ev8ylYnkKBEqnB'                                     // Real voice ID from config.ts
            }
        };

        // Simulate VapiService functionality
        class BrowserVapiService {
            constructor(config) {
                this.config = config;
                this.websocket = null;
                this.isConnected = false;
                this.eventListeners = {};
            }

            on(event, callback) {
                if (!this.eventListeners[event]) {
                    this.eventListeners[event] = [];
                }
                this.eventListeners[event].push(callback);
            }

            emit(event, data) {
                if (this.eventListeners[event]) {
                    this.eventListeners[event].forEach(callback => callback(data));
                }
            }

            async startListening() {
                log('üé§ Starting speech recognition...');
                log('1. Requesting microphone access...');

                try {
                    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                    log('‚úÖ Microphone access granted');

                    log('2. Creating Vapi call...');
                    const response = await fetch('https://api.vapi.ai/call', {
                        method: 'POST',
                        headers: {
                            'Authorization': `Bearer ${this.config.apiKey}`,
                            'Content-Type': 'application/json'
                        },
                        body: JSON.stringify({
                            assistantId: this.config.assistantId,
                            transport: {
                                provider: 'vapi.websocket',
                                audioFormat: {
                                    format: 'pcm_s16le',
                                    container: 'raw',
                                    sampleRate: 16000
                                }
                            }
                        })
                    });

                    if (!response.ok) {
                        throw new Error(`Failed to create call: ${response.status}`);
                    }

                    const callData = await response.json();
                    log(`‚úÖ Call created: ${callData.id}`);

                    log('3. Connecting to Vapi WebSocket...');
                    return new Promise((resolve, reject) => {
                        this.websocket = new WebSocket(callData.transport.websocketCallUrl);

                        this.websocket.onopen = () => {
                            log('‚úÖ WebSocket connected');
                            this.isConnected = true;
                            this.startAudioStreaming(stream);
                            resolve();
                        };

                        this.websocket.onmessage = (event) => {
                            this.handleMessage(event);
                        };

                        this.websocket.onerror = (error) => {
                            log(`‚ùå WebSocket error: ${error}`, 'error');
                            reject(error);
                        };

                        this.websocket.onclose = () => {
                            log('üîå WebSocket disconnected');
                            this.isConnected = false;
                        };
                    });

                } catch (error) {
                    log(`‚ùå Failed to start: ${error.message}`, 'error');
                    throw error;
                }
            }

            handleMessage(event) {
                try {
                    // Check if this is binary audio data (Blob or ArrayBuffer)
                    if (event.data instanceof Blob || event.data instanceof ArrayBuffer) {
                        // This is audio data from Vapi - ignore it for STT-only mode
                        return;
                    }

                    // Only try to parse text messages as JSON
                    if (typeof event.data !== 'string') {
                        log(`Ignoring non-text message: ${typeof event.data}`);
                        return;
                    }

                    const message = JSON.parse(event.data);

                    // Log raw message (truncated)
                    const logMsg = JSON.stringify(message).substring(0, 100);
                    log(`üì• Raw message: ${logMsg}${JSON.stringify(message).length > 100 ? '...' : ''}`);

                    switch (message.type) {
                        case 'transcript':
                            if (message.role === 'user') {
                                const transcript = message.transcript || '';
                                const type = message.transcriptType || 'unknown';
                                log(`üìù USER Transcript: "${transcript}" (${type})`, 'success');

                                // Update transcript display
                                const transcriptDisplay = document.getElementById('transcriptDisplay');
                                const transcriptCard = document.getElementById('transcriptCard');

                                transcriptDisplay.textContent = transcript;
                                transcriptDisplay.className = 'transcript-text';
                                transcriptCard.classList.add('has-content');

                                // Emit transcription event
                                this.emit('transcription', {
                                    text: transcript,
                                    isFinal: type === 'final',
                                    confidence: 0.9,
                                    timestamp: Date.now()
                                });
                            } else {
                                log(`ü§ñ Assistant transcript (ignored): "${message.transcript}"`);
                            }
                            break;

                        case 'speech-update':
                            if (message.role === 'user') {
                                log(`üó£Ô∏è Speech ${message.status}`, message.status === 'started' ? 'success' : 'warning');
                            }
                            break;

                        case 'status-update':
                            log(`üìä Status: ${message.status}`);
                            break;

                        case 'model-output':
                            // Suppress to prevent spam
                            break;

                        case 'conversation-update':
                            // Suppress to prevent spam
                            break;

                        default:
                            log(`üì• Unknown message type: ${message.type}`);
                            break;
                    }
                } catch (error) {
                    log(`‚ùå Failed to parse message: ${error}`, 'error');
                }
            }

            startAudioStreaming(stream) {
                const audioContext = new AudioContext({ sampleRate: 16000 });
                const source = audioContext.createMediaStreamSource(stream);
                const processor = audioContext.createScriptProcessor(4096, 1, 1);

                log(`üìä Audio context sample rate: ${audioContext.sampleRate}Hz`);

                processor.onaudioprocess = (event) => {
                    if (this.websocket && this.websocket.readyState === WebSocket.OPEN) {
                        const inputData = event.inputBuffer.getChannelData(0);
                        const pcmData = new Int16Array(inputData.length);

                        for (let i = 0; i < inputData.length; i++) {
                            pcmData[i] = Math.max(-32768, Math.min(32767, inputData[i] * 32768));
                        }

                        try {
                            this.websocket.send(pcmData.buffer);
                        } catch (error) {
                            log(`‚ùå Failed to send audio: ${error}`, 'error');
                        }
                    }
                };

                source.connect(processor);
                processor.connect(audioContext.destination);

                this.audioContext = audioContext;
                this.audioSource = source;
                this.audioProcessor = processor;
                this.mediaStream = stream;
            }

            async stopListening() {
                log('‚èπÔ∏è Stopping speech recognition...');

                if (this.audioProcessor) {
                    this.audioProcessor.disconnect();
                }
                if (this.audioSource) {
                    this.audioSource.disconnect();
                }
                if (this.audioContext) {
                    await this.audioContext.close();
                }
                if (this.mediaStream) {
                    this.mediaStream.getTracks().forEach(track => track.stop());
                }
                if (this.websocket) {
                    this.websocket.close();
                }

                this.isConnected = false;
                log('‚úÖ Stopped listening');
            }
        }

        // Simulate TTSService functionality
        class BrowserTTSService {
            constructor(config) {
                this.config = config;
            }

                        async synthesizeText(text, options = {}) {
                log(`üó£Ô∏è Synthesizing: "${text}"`);

                try {
                    // Use the same request structure as the real TTSService
                    const voiceSettings = {
                        stability: 0.75,
                        similarity_boost: 0.8,
                        style: 0.2,
                        use_speaker_boost: true
                    };

                    const requestBody = {
                        text: text,
                        model_id: 'eleven_turbo_v2',
                        voice_settings: voiceSettings
                    };

                    // Use streaming endpoint like the real TTSService
                    const response = await fetch(`https://api.elevenlabs.io/v1/text-to-speech/${this.config.voiceId}/stream`, {
                        method: 'POST',
                        headers: {
                            'Accept': 'audio/mpeg',
                            'Content-Type': 'application/json',
                            'xi-api-key': this.config.apiKey
                        },
                        body: JSON.stringify(requestBody)
                    });

                    if (!response.ok) {
                        const errorText = await response.text();
                        log(`‚ùå TTS API Error: ${response.status} ${response.statusText} - ${errorText}`, 'error');
                        throw new Error(`TTS failed: ${response.status} ${response.statusText}`);
                    }

                    const audioBlob = await response.blob();
                    log('‚úÖ TTS synthesis complete');

                    return {
                        audioStream: audioBlob,
                        duration: 0, // We don't know duration without decoding
                        format: 'mp3',
                        sampleRate: 22050 // ElevenLabs default
                    };

                } catch (error) {
                    log(`‚ùå TTS failed: ${error.message}`, 'error');
                    throw error;
                }
            }

            async playAudio(audioBlob) {
                return new Promise((resolve, reject) => {
                    const audio = new Audio();
                    const url = URL.createObjectURL(audioBlob);

                    audio.onloadeddata = () => {
                        log(`üîä Playing TTS audio (${Math.round(audio.duration)}s)`);
                    };

                    audio.onended = () => {
                        URL.revokeObjectURL(url);
                        log('‚úÖ TTS playback complete');
                        resolve();
                    };

                    audio.onerror = (error) => {
                        URL.revokeObjectURL(url);
                        log(`‚ùå TTS playback failed: ${error}`, 'error');
                        reject(error);
                    };

                    audio.src = url;
                    audio.play();
                });
            }
        }

        // Initialize services
        vapiService = new BrowserVapiService(CONFIG.vapi);
        ttsService = new BrowserTTSService(CONFIG.elevenlabs);

        // Set up event listeners
        vapiService.on('transcription', (transcription) => {
            if (transcription.isFinal) {
                log(`üéØ Final transcript: "${transcription.text}"`, 'success');

                // Simulate AI response (this is where your team's LangGraph would go)
                const mockResponses = [
                    "That's an interesting question about the story.",
                    "The main character is quite complex in this chapter.",
                    "This part of the book explores themes of identity and growth.",
                    "The author's writing style really shines in this section."
                ];

                const response = mockResponses[Math.floor(Math.random() * mockResponses.length)];

                setTimeout(async () => {
                    log(`üß† Mock AI Response: "${response}"`);

                    try {
                        const ttsResult = await ttsService.synthesizeText(response);
                        await ttsService.playAudio(ttsResult.audioStream);
                        log('üéâ Complete pipeline test successful!', 'success');
                    } catch (error) {
                        log(`‚ùå TTS pipeline failed: ${error.message}`, 'error');
                    }
                }, 1000);
            }
        });

        // Global functions for buttons
        window.startVoicePipeline = async function() {
            try {
                document.getElementById('startBtn').disabled = true;
                document.getElementById('stopBtn').disabled = false;
                isListening = true;

                await vapiService.startListening();
                updateStatus('üé§ Listening for speech... Try saying something!', 'success');

            } catch (error) {
                log(`‚ùå Failed to start pipeline: ${error.message}`, 'error');
                updateStatus('‚ùå Failed to start pipeline', 'error');
                document.getElementById('startBtn').disabled = false;
                document.getElementById('stopBtn').disabled = true;
                isListening = false;
            }
        };

        window.stopVoicePipeline = async function() {
            try {
                await vapiService.stopListening();
                document.getElementById('startBtn').disabled = false;
                document.getElementById('stopBtn').disabled = true;
                isListening = false;
                updateStatus('‚èπÔ∏è Pipeline stopped', 'warning');
                const transcriptDisplay = document.getElementById('transcriptDisplay');
                const transcriptCard = document.getElementById('transcriptCard');

                transcriptDisplay.textContent = 'Speak to see live transcription and AI responses...';
                transcriptDisplay.className = 'transcript-placeholder';
                transcriptCard.classList.remove('has-content');

            } catch (error) {
                log(`‚ùå Failed to stop pipeline: ${error.message}`, 'error');
            }
        };

        window.testTTS = async function() {
            try {
                const testText = "Hello! This is a test of the ElevenLabs text-to-speech service. The voice pipeline is working correctly.";
                const result = await ttsService.synthesizeText(testText);
                await ttsService.playAudio(result.audioStream);

            } catch (error) {
                log(`‚ùå TTS test failed: ${error.message}`, 'error');
            }
        };

        window.clearLog = function() {
            document.getElementById('log').innerHTML = '';
        };

        // Utility functions
        function log(message, type = 'info') {
            const timestamp = new Date().toLocaleTimeString();
            const logElement = document.getElementById('log');
            const className = type === 'error' ? 'error' : type === 'success' ? 'success' : type === 'warning' ? 'warning' : '';

            logElement.innerHTML += `<div class="${className}">[${timestamp}] ${message}</div>`;
            logElement.scrollTop = logElement.scrollHeight;
        }

        function updateStatus(message, type = 'info') {
            const statusElement = document.getElementById('status');
            const statusDot = document.getElementById('statusDot');

            // Update status text
            statusElement.textContent = message;

            // Update status dot
            statusDot.className = 'status-dot';
            if (type === 'error') {
                statusDot.classList.add('error');
            } else if (type === 'success' || type === 'ready') {
                statusDot.classList.add('ready');
            } else if (type === 'listening') {
                statusDot.classList.add('listening');
            } else if (type === 'processing') {
                statusDot.classList.add('processing');
            } else {
                statusDot.classList.add('ready');
            }
        }

        // Initialize
        log('üé§ Real Services Test Ready');
        log('Click "Start Voice Pipeline" to begin testing with your actual VapiService and TTSService');
        updateStatus('Ready to test real services...', 'success');
    </script>
</body>
</html>
